{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scripts.feature_extractor import sent_to_features, sent_to_labels\n",
    "from scripts.preprocessor import clean_arabic\n",
    "import pickle\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_codes_1 = ['chr_position', 'minus5', 'minus4', 'minus3', 'minus2', 'minus1', 'focus',\n",
    "                 'plus1', 'plus2', 'plus3', 'plus4', 'plus5', 'next2letters', \n",
    "                 'prev2letters', 'prev_word_suffix', 'following_word_prefix',\n",
    "                 'focus_word_prefix', 'focus_word_suffix']\n",
    "\n",
    "feature_codes_2 = ['minus5', 'minus4', 'minus3', 'minus2', 'minus1', 'focus',\n",
    "                 'plus1', 'plus2', 'plus3', 'plus4', 'plus5',\n",
    "                   'prev_word_minus1', 'prev_word_minus2', 'prev_word_minus3',\n",
    "                  'following_word_plus0', 'following_word_plus1', 'following_word_plus2']\n",
    "\n",
    "feature_codes = {'fc1': feature_codes_1, 'fc2': feature_codes_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_featureset(dataframe, feature_codes, out_file_path):\n",
    "    \"\"\"\n",
    "    This functions takes in a tsv file with 4 columns: file, sentence_no, raw and seg\n",
    "    and creates a featureset out of it. \n",
    "    \"\"\"\n",
    "    if feature_codes == 'fc1':\n",
    "        feature_names = feature_codes_1\n",
    "    elif feature_codes == 'fc2':\n",
    "        feature_names = feature_codes_2\n",
    "    else:\n",
    "        raise Exception(\"Feature code not supported\")\n",
    "        \n",
    "    with open(out_file_path, 'w') as out_file:\n",
    "        fwriter = csv.writer(out_file, delimiter='\\t')\n",
    "        fwriter.writerow([\"file\", \"sentence_no\", \"word_no\", \"word\", \"char\"] + feature_names + [\"char_label\"])\n",
    "        for i in range(len(dataframe)):\n",
    "            raw_sent = dataframe['raw'][i]\n",
    "            seg_sent = dataframe['seg'][i]\n",
    "            sent_feats, sent_labels = sent_to_features(raw_sent, feature_names), sent_to_labels(seg_sent)\n",
    "            for word_no, (word, word_feats, word_labels) in enumerate(zip(raw_sent.split(), sent_feats, sent_labels)):\n",
    "                for char, char_feats, char_label in zip(word, word_feats, word_labels):\n",
    "                    fwriter.writerow([dataframe['file'][i], i, word_no, word, char] + char_feats + [char_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_featureset(in_tsv_path, raw_col, seg_col, feature_code, out_file_path):\n",
    "    \"\"\"\n",
    "    This function takes as input a tsv file which contains the raw and segmented form of sentence.\n",
    "    raw_col is the name of the column containing raw sentences\n",
    "    seg_col is the name of the column containing seg sentences\n",
    "    feature_code = 'fc1' or 'fc2'\n",
    "    The path of the output folder. It will append '_{feature_code}' to the input file name to produce\n",
    "    the output file name\n",
    "    \"\"\"\n",
    "    feature_names = feature_codes[feature_code]\n",
    "    \n",
    "    with open(in_tsv_path) as infile, open(out_file_path, 'w') as outfile:\n",
    "        freader = csv.DictReader(infile, delimiter='\\t')\n",
    "        fwriter = csv.writer(outfile, delimiter='\\t')\n",
    "        fwriter.writerow([\"file\", \"sentence_no\", \"word_no\", \"word\", \"char\"] + feature_names + [\"char_label\"])\n",
    "        for line_no, line in enumerate(freader):\n",
    "            raw_sent = line[raw_col]\n",
    "            sent_feats = sent_to_features(raw_sent, feature_names)\n",
    "            if seg_col:\n",
    "                seg_sent = line[seg_col]\n",
    "                sent_labels = sent_to_labels(seg_sent)\n",
    "            \n",
    "            sent_feats, sent_labels = sent_to_features(raw_sent, feature_names), sent_to_labels(seg_sent)\n",
    "            for word_no, (word, word_feats, word_labels) in enumerate(zip(raw_sent.split(), sent_feats, sent_labels)):\n",
    "                for char, char_feats, char_label in zip(word, word_feats, word_labels):\n",
    "                    fwriter.writerow([line['file'], line_no, word_no, word, char] + char_feats + [char_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A new test set was provided by Emad which will help us compare our segmenter with Mada-Mira and Farasa.\n",
    "Emad has results of running the above two segmenters on this test set.\n",
    "Location: data/segmenter/raw/test2.txt (File 1)\n",
    "\n",
    "More segmentation data was provided by Emad.\n",
    "Location: data/segmenter/raw/all_my_segmentation_data.corrected (File 2)\n",
    "\n",
    "Note: There is overlap between File 1 and File 2.\n",
    "\n",
    "From the original Al-Mannar corpus, we have already carved out a train, dev and test.\n",
    "The processed versions of these named train1.tsv, dev1.tsv and test1.tsv are stored in data/segmenter/raw\n",
    "\n",
    "In this codeblock, we will find the overlap between the two files and add the non-overlapping sentences from \n",
    "File 2 to the original train and make a new train2.tsv. \n",
    "The original dev1.tsv will remain as is and we will have a new test set called test2.tsv\n",
    "\n",
    "Eventually, we shall have the following:\n",
    "train1.tsv, train2.tsv\n",
    "dev1.tsv\n",
    "test1.tsv, test2.tsv\n",
    "\"\"\"\n",
    "test2_set = set()\n",
    "with open('data/segmenter/raw/test2.txt') as test2file:\n",
    "    for line in test2file:\n",
    "        test2_set.add(line.strip())\n",
    "        \n",
    "allseg_set = set()\n",
    "with open('data/segmenter/raw/all_my_segmentation_data.corrected') as allsegfile:\n",
    "    for line in allsegfile:\n",
    "        allseg_set.add(line.strip())\n",
    "        \n",
    "with open('data/segmenter/raw/test2.tsv', 'w') as test2tsv:\n",
    "    fwriter = csv.writer(test2tsv, delimiter='\\t')\n",
    "    fwriter.writerow(['file', 'sentence', 'raw', 'seg'])\n",
    "    for sentence_no, seg in enumerate(test2_set):\n",
    "        raw = ' '.join([''.join(word.split('+')) for word in seg.split(' ')])\n",
    "        fwriter.writerow(['test2', sentence_no, raw, seg])\n",
    "\n",
    "with open('data/segmenter/raw/train2.tsv', 'w') as train2tsv, open('data/segmenter/raw/train1.tsv') as train1tsv:\n",
    "    fwriter = csv.writer(train2tsv, delimiter='\\t')\n",
    "    fwriter.writerow(['file', 'sentence', 'raw', 'seg'])\n",
    "    for sentence_no, seg in enumerate((allseg_set - test2_set)):\n",
    "        if seg:\n",
    "            raw = ' '.join([''.join(word.split('+')) for word in seg.split(' ')])\n",
    "            fwriter.writerow(['all_my_seg_data', sentence_no, raw, seg])\n",
    "    \n",
    "    freader = csv.reader(train1tsv, delimiter='\\t')\n",
    "    next(freader, None)\n",
    "    for line in freader:\n",
    "        fwriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There were errors found in test1 or Manar Test. So Emad corrected them\n",
    "and sent me a file called manar_corrected.test. Let us process it \n",
    "and save it in test3.test\n",
    "\"\"\"\n",
    "with open('data/segmenter/raw/test3.tsv', 'w') as test3tsv, \\\n",
    "        open('data/segmenter/raw/manar_corrected.test', 'r') as manar_test:\n",
    "    \n",
    "    fwriter = csv.writer(test3tsv, delimiter='\\t')\n",
    "    fwriter.writerow(['file', 'sentence', 'raw', 'seg'])\n",
    "    for sentence_no, seg in enumerate(manar_test):\n",
    "        seg = seg.strip()\n",
    "        raw = ' '.join([''.join(word.split('+')) for word in seg.split(' ')])\n",
    "        fwriter.writerow(['test2', sentence_no, raw, seg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2443\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2471\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2509\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9172\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The train2 created above apparently had sentences from ATB which were present in \n",
    "all_my_segmentation_data. So, we area creating a train3 by adding more stuff from \n",
    "a new folder called classical_train. ATB data has been removed from this.\n",
    "\"\"\"\n",
    "i = 0\n",
    "files = os.listdir(\"data/segmenter/raw/classical_train/\")\n",
    "with open('data/segmenter/raw/train4.tsv', 'w') as train3tsv, open('data/segmenter/raw/train1.tsv') as train1tsv:\n",
    "    fwriter = csv.writer(train3tsv, delimiter='\\t')\n",
    "    fwriter.writerow(['file', 'sentence', 'raw', 'seg'])\n",
    "    for rfile_name in files:\n",
    "        with open(os.path.join(\"data/segmenter/raw/classical_train/\", rfile_name)) as rfile:\n",
    "            for sentence_no, seg in enumerate(rfile):\n",
    "                seg = seg.strip()\n",
    "                if seg:\n",
    "                    raw = ' '.join([''.join(word.split('+')) for word in seg.split(' ')])\n",
    "                    fwriter.writerow([rfile_name, sentence_no, raw, seg])\n",
    "                    i += 1\n",
    "            print(i)\n",
    "            input()\n",
    "    freader = csv.reader(train1tsv, delimiter='\\t')\n",
    "    next(freader, None)\n",
    "    for line in freader:\n",
    "        fwriter.writerow(line)\n",
    "        i += 1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on train4.tsv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following piece of code will call the create_featureset() function for dev/train/test sets.\n",
    "You just need to change the `set_no` and the `feature_code`.\n",
    "\"\"\"\n",
    "# Set the following variables.\n",
    "feature_code = 'fc1'\n",
    "in_folder = 'data/segmenter/raw'\n",
    "out_folder = 'data/segmenter/processed/'\n",
    "\n",
    "# file_sets = ['dev', 'test', 'train']\n",
    "# file_names = [i + str(set_no) + '.tsv' for i in file_sets]\n",
    "# file_names = ['train1_sso.tsv', 'dev1_sso.tsv', 'test1_sso.tsv']\n",
    "# file_names = [file for file in os.listdir(in_folder) if file.endswith('.tsv')]\n",
    "file_names = ['train4.tsv']\n",
    "\n",
    "for f in file_names:\n",
    "    print('Working on {}'.format(f))\n",
    "    frame = pd.read_csv(os.path.join(in_folder, f), delimiter='\\t')\n",
    "    outfilename = f.split('.')[0] + '_' + feature_code + '.tsv'\n",
    "    create_featureset(frame, feature_code, os.path.join(out_folder, outfilename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code will convert the segmentation data to substandard orthography\n",
    "The will enable us to study the effect of segmention on standard and substandard data\n",
    "\n",
    "The files will be read from in_folder and writted into out_folder\n",
    "\"\"\"\n",
    "\n",
    "in_folder = 'data/segmenter/raw'\n",
    "out_folder = 'data/combined/raw'\n",
    "file_names = [file for file in os.listdir(in_folder) if file.endswith('.tsv')]\n",
    "file_names = ['train4.tsv']\n",
    "substandard_dict = {\n",
    "    'أ': 'ا',\n",
    "    'إ': 'ا',\n",
    "    'آ': 'ا',\n",
    "    'ة': 'ه',\n",
    "}\n",
    "\n",
    "def substandardize(somestring):\n",
    "    out = [substandard_dict.get(letter, letter) for letter in somestring]\n",
    "    return ''.join(out)\n",
    "\n",
    "def replacey(word):\n",
    "    if word.endswith('ي'):\n",
    "        new_word = word[:-1] + 'ى'\n",
    "    else:\n",
    "        new_word = word\n",
    "    return new_word\n",
    "\n",
    "\n",
    "for file in file_names:\n",
    "    outfilename = file.split('.')[0] + '_sso.tsv'\n",
    "    with open(os.path.join(in_folder, file)) as infile, open(os.path.join(out_folder, outfilename), 'w') as outfile:\n",
    "        freader = csv.DictReader(infile, delimiter='\\t')\n",
    "        fwriter = csv.writer(outfile, delimiter='\\t')\n",
    "        fwriter.writerow(['file', 'sentence', 'original_raw', 'original_seg', 'sso_raw', 'sso_seg'])\n",
    "        for line in freader:\n",
    "            raw_line = line['raw']\n",
    "            raw_sso_line = \" \".join([replacey(w) for w in substandardize(raw_line).split()])\n",
    "            seg_line = line['seg']\n",
    "            seg_sso_line = \" \".join([replacey(w) for w in substandardize(seg_line).split()])\n",
    "            fwriter.writerow([line['file'], line['sentence'], raw_line, seg_line, raw_sso_line, seg_sso_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on train4_sso.tsv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following piece of code will call the create_featureset() function for SUBSTANDARD dev/train/test sets.\n",
    "You just need to change the `set_no` and the `feature_code`.\n",
    "\"\"\"\n",
    "# Set the following variables.\n",
    "feature_code = 'fc1'\n",
    "in_folder = 'data/combined/raw'\n",
    "out_folder = 'data/combined/processed/'\n",
    "raw_col = 'sso_raw'\n",
    "seg_col = 'sso_seg'\n",
    "\n",
    "# file_sets = ['dev', 'test', 'train']\n",
    "# file_names = [i + str(set_no) + '.tsv' for i in file_sets]\n",
    "# file_names = ['train1_sso.tsv', 'dev1_sso.tsv', 'test1_sso.tsv']\n",
    "# file_names = ['test3.tsv', 'train3.tsv']\n",
    "# file_names = [file for file in os.listdir(in_folder) if file.endswith('.tsv')]\n",
    "file_names = ['train4_sso.tsv']\n",
    "\n",
    "for f in file_names:\n",
    "    print('Working on {}'.format(f))\n",
    "    out_file_name = f.split('.')[0] + '_' + feature_code + '.tsv'\n",
    "    out_file_path = os.path.join(out_folder, out_file_name)\n",
    "    create_featureset(os.path.join(in_folder, f), raw_col, seg_col, feature_code, out_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
