{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import segmenter\n",
    "import csv\n",
    "from scripts.preprocessor import clean_arabic\n",
    "import pandas as pd\n",
    "from scripts.feature_extractor import char_to_features\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "substandard_dict = {\n",
    "    'أ': 'ا',\n",
    "    'إ': 'ا',\n",
    "    'آ': 'ا',\n",
    "    'ة': 'ه',\n",
    "}\n",
    "\n",
    "def substandardize(somestring):\n",
    "    out = [substandard_dict.get(letter, letter) for letter in somestring]\n",
    "    return ''.join(out)\n",
    "\n",
    "def replacey(word):\n",
    "    if word.endswith('ي'):\n",
    "        new_word = word[:-1] + 'ى'\n",
    "    else:\n",
    "        new_word = word\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_codes_1 = ['chr_position', 'minus5', 'minus4', 'minus3', 'minus2', 'minus1', 'focus',\n",
    "                 'plus1', 'plus2', 'plus3', 'plus4', 'plus5', 'next2letters', \n",
    "                 'prev2letters', 'prev_word_suffix', 'following_word_prefix',\n",
    "                 'focus_word_prefix', 'focus_word_suffix']\n",
    "\n",
    "feature_codes_2 = ['minus5', 'minus4', 'minus3', 'minus2', 'minus1', 'focus',\n",
    "                 'plus1', 'plus2', 'plus3', 'plus4', 'plus5',\n",
    "                   'prev_word_minus1', 'prev_word_minus2', 'prev_word_minus3',\n",
    "                  'following_word_plus0', 'following_word_plus1', 'following_word_plus2']\n",
    "\n",
    "feature_codes = {'fc1': feature_codes_1, 'fc2': feature_codes_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code converts a raw text file into a tsv\n",
    "# The tsv has standard and substandard sentences side by side\n",
    "\n",
    "#filename = \"data/sso/islamweb_articles.txt\"\n",
    "filename = \"data/sso/raw/siyar.txt\"\n",
    "outfilename = \"data/sso/processed/\" + os.path.split(filename)[1].split('.')[0] + '.tsv'\n",
    "\n",
    "with open(outfilename, 'w') as outfile:\n",
    "    fwriter = csv.writer(outfile, delimiter='\\t')\n",
    "    fwriter.writerow([\"standard\", \"substandard\"])\n",
    "    infile = open(filename)\n",
    "    for line in infile:\n",
    "        line = clean_arabic(line)\n",
    "        line = \" \".join(line.strip().split())\n",
    "        sso_line = \" \".join([replacey(w) for w in substandardize(line).split()])\n",
    "        if len(line) != len(sso_line):\n",
    "            print(len(line), len(sso_line))\n",
    "            raise Exception(\"Lengths are not equal\")\n",
    "        fwriter.writerow([line, sso_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the raw sso.tsv to a feature frame.\n",
    "\"\"\"\n",
    "raw_folder = \"data/standardizer/raw\"\n",
    "processed_folder = \"data/standardizer/processed\"\n",
    "fc = 'fc1'\n",
    "files = ['albidya_walnihaya.tsv', 'islamweb_articles.tsv', 'islamweb_fatwa_answers.tsv', 'siyar.tsv']\n",
    "symbols = set(('ا', 'ه'))\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    infilepath = os.path.join(raw_folder, file)\n",
    "    outfilepath = os.path.join(processed_folder, file.split('.')[0] + '_' + fc + '.tsv')\n",
    "    inframe = pd.read_csv(infilepath, sep='\\t')\n",
    "\n",
    "    problem_lines = []\n",
    "\n",
    "    with open(outfilepath, 'w') as outfile:\n",
    "        fwriter = csv.writer(outfile, delimiter='\\t')\n",
    "        fwriter.writerow(['sentence_no', 'word_no', 'char_no'] + feature_codes[fc] + ['target'])\n",
    "        for i, sent in enumerate(inframe['substandard']):\n",
    "            try:\n",
    "                sent = sent.split()\n",
    "                for word_pos, word in enumerate(sent):\n",
    "                    for char_pos, char in enumerate(word):\n",
    "                        if char in symbols or (char_pos == len(word)-1 and char == 'ى'):\n",
    "                            features = char_to_features(char_pos, word_pos, sent, feature_codes[fc])\n",
    "                            target = [inframe['standard'][i].split()[word_pos][char_pos]]\n",
    "                            fwriter.writerow([i, word_pos, char_pos] + features + target)\n",
    "            except AttributeError:\n",
    "                problem_lines.append(i)\n",
    "\n",
    "    print(problem_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is completely hardcoded and only merges the four files\n",
    "# using FC1. Please be careful before using it for other featuresets.\n",
    "# OUTPUT: data1_fc1.tsc inside data/standardizer/sets\n",
    "# This is all 4 files put together\n",
    "sso_fc1 = pd.read_csv(\"data/standardizer/processed/islamweb_fatwa_answers_fc1.tsv\", sep='\\t')\n",
    "sso_fc2 = pd.read_csv(\"data/standardizer/processed/islamweb_articles_fc1.tsv\", sep='\\t')\n",
    "sso_fc3 = pd.read_csv(\"data/standardizer/processed/siyar_fc1.tsv\", sep='\\t')\n",
    "sso_fc4 = pd.read_csv(\"data/standardizer/processed/albidya_walnihaya_fc1.tsv\", sep='\\t')\n",
    "data = pd.concat([sso_fc1, sso_fc2, sso_fc3, sso_fc4])\n",
    "data.to_csv('data/standardizer/sets/data1_fc1.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code random undersamples the majority class i.e. 'ا'\n",
    "# down to 7 million samples and saves the new dataframe as 'data2'\n",
    "data = pd.read_csv('data/standardizer/sets/data1_fc1.tsv', sep='\\t')\n",
    "alif_df = data[data['target']=='ا'].sample(n=7000000, random_state=42)\n",
    "non_alif_df = data[data['target']!='ا']\n",
    "data2 = pd.concat([alif_df, non_alif_df])\n",
    "data2.to_csv('data/standardizer/sets/data2_fc1.tsv', sep='\\t')\n",
    "data = data2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ا    7000000\n",
       "ه    6877584\n",
       "أ    3847878\n",
       "ة    2653253\n",
       "ي    2310738\n",
       "ى    1975402\n",
       "إ    1612944\n",
       "آ     330392\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>output</th>\n",
       "      <th>آ</th>\n",
       "      <th>أ</th>\n",
       "      <th>إ</th>\n",
       "      <th>ا</th>\n",
       "      <th>ة</th>\n",
       "      <th>ه</th>\n",
       "      <th>ى</th>\n",
       "      <th>ي</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>input</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ا</th>\n",
       "      <td>330392</td>\n",
       "      <td>3847878</td>\n",
       "      <td>1612944</td>\n",
       "      <td>7000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ه</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2653253</td>\n",
       "      <td>6877584</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ى</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1975402</td>\n",
       "      <td>2310738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "output       آ        أ        إ        ا        ة        ه        ى        ي\n",
       "input                                                                        \n",
       "ا       330392  3847878  1612944  7000000        0        0        0        0\n",
       "ه            0        0        0        0  2653253  6877584        0        0\n",
       "ى            0        0        0        0        0        0  1975402  2310738"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data['focus'], data['target'], rownames=['input'], colnames=['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ا    19846338\n",
       "ه     6877584\n",
       "أ     3847878\n",
       "ة     2653253\n",
       "ي     2310738\n",
       "ى     1975402\n",
       "إ     1612944\n",
       "آ      330392\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_ratios = [(0.3, 0.5), (0.1, 0.5)]\n",
    "\n",
    "data = pd.read_csv('data/standardizer/sets/data2_fc1.tsv', sep='\\t')\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, temp_index in sss.split(data, data['target']):\n",
    "    train = data.iloc[train_index]\n",
    "    temp = data.iloc[temp_index]\n",
    "    \n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "for dev_index, test_index in sss.split(temp, temp['target']):\n",
    "    dev = temp.iloc[dev_index]\n",
    "    test = temp.iloc[test_index]\n",
    "    \n",
    "train.to_csv('data/standardizer/sets/train1_fc1.tsv', sep='\\t', index=True, header=True)\n",
    "dev.to_csv('data/standardizer/sets/dev1_fc1.tsv', sep='\\t', index=True, header=True)\n",
    "test.to_csv('data/standardizer/sets/test1_fc1.tsv', sep='\\t', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsayyed/Software/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/standardizer/sets/train1_fc1.tsv', sep='\\t')\n",
    "dev = pd.read_csv('data/standardizer/sets/dev1_fc1.tsv', sep='\\t')\n",
    "test = pd.read_csv('data/standardizer/sets/test1_fc1.tsv', sep='\\t')\n",
    "fc = 'fc1'\n",
    "\n",
    "Xtrain, ytrain = train[feature_codes[fc]], train['target']\n",
    "Xdev, ydev = dev[feature_codes[fc]], dev['target']\n",
    "Xtest, ytest = test[feature_codes[fc]], test['target']\n",
    "categorical_features_indices = np.where(Xtrain.dtypes != np.float)[0]\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(ytrain)\n",
    "ytrain = le.transform(ytrain)\n",
    "ydev = le.transform(ydev)\n",
    "ytest = le.transform(ytest)\n",
    "np.save('models/sso_encoder_classes.npy', le.classes_)\n",
    "\n",
    "frames = [Xtrain, Xdev, Xtest]\n",
    "for frame in frames:\n",
    "    for c in frame.columns[1:]:\n",
    "        frame[c] = frame[c].astype('category')\n",
    "        \n",
    "lgb_train = lgb.Dataset(Xtrain, ytrain)\n",
    "lgb_eval = lgb.Dataset(Xdev, ydev, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['آ' 'أ' 'إ' 'ا' 'ة' 'ه' 'ى' 'ي']\n",
      "(18625733,) (18625733, 18) (3991229, 18) (3991229,) (3991229,)\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n",
    "print(ytrain.shape, Xtrain.shape, Xdev.shape , ydev.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 8,\n",
    "    'metric': 'multi_logloss',\n",
    "    'train_metric': True,\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'num_threads': 7,\n",
    "}\n",
    "\n",
    "# model = lgb.train(params, \n",
    "#                 lgb_train,\n",
    "#                 num_boost_round=1500,\n",
    "#                 valid_sets=[lgb_train, lgb_eval],\n",
    "#                 early_stopping_rounds=30)\n",
    "\n",
    "model = lgb.cv(params,\n",
    "               lgb_train,\n",
    "               num_boost_round=1500,\n",
    "               early_stopping_rounds=40\n",
    "               #metrics=['multi_logloss', 'multi_error']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    thread_count=7,\n",
    "    eval_metric=['TotalF1'],\n",
    "    loss_function='MultiClass',\n",
    "    logging_level='Verbose'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    Xtrain, ytrain,\n",
    "    cat_features=categorical_features_indices,\n",
    "    eval_set=(Xdev, ydev),\n",
    "    plot=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"data/standardizer/models/cb_set1_1000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9864751631519683 0.9860178406200195 0.9859607153586025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97    231274\n",
      "           1       0.98      0.98      0.98   2693514\n",
      "           2       0.97      0.96      0.96   1129061\n",
      "           3       1.00      1.00      1.00   4900000\n",
      "           4       0.98      0.98      0.98   1857277\n",
      "           5       0.99      0.99      0.99   4814309\n",
      "           6       0.98      0.98      0.98   1382781\n",
      "           7       0.98      0.99      0.98   1617517\n",
      "\n",
      "   micro avg       0.99      0.99      0.99  18625733\n",
      "   macro avg       0.98      0.98      0.98  18625733\n",
      "weighted avg       0.99      0.99      0.99  18625733\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     49559\n",
      "           1       0.98      0.98      0.98    577182\n",
      "           2       0.97      0.96      0.96    241941\n",
      "           3       1.00      1.00      1.00   1050000\n",
      "           4       0.98      0.98      0.98    397988\n",
      "           5       0.99      0.99      0.99   1031638\n",
      "           6       0.98      0.98      0.98    296310\n",
      "           7       0.98      0.98      0.98    346611\n",
      "\n",
      "   micro avg       0.99      0.99      0.99   3991229\n",
      "   macro avg       0.98      0.98      0.98   3991229\n",
      "weighted avg       0.99      0.99      0.99   3991229\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     49559\n",
      "           1       0.98      0.98      0.98    577182\n",
      "           2       0.97      0.96      0.96    241942\n",
      "           3       1.00      1.00      1.00   1050000\n",
      "           4       0.98      0.98      0.98    397988\n",
      "           5       0.99      0.99      0.99   1031637\n",
      "           6       0.98      0.98      0.98    296311\n",
      "           7       0.98      0.98      0.98    346610\n",
      "\n",
      "   micro avg       0.99      0.99      0.99   3991229\n",
      "   macro avg       0.98      0.98      0.98   3991229\n",
      "weighted avg       0.99      0.99      0.99   3991229\n",
      "\n",
      "[[ 222905    5856     808    1705       0       0       0       0]\n",
      " [   2296 2641452   31868   17898       0       0       0       0]\n",
      " [    572   41627 1084434    2428       0       0       0       0]\n",
      " [    469   14768    2243 4882520       0       0       0       0]\n",
      " [      0       0       0       0 1819398   37879       0       0]\n",
      " [      0       0       0       0   40116 4774193       0       0]\n",
      " [      0       0       0       0       0       0 1355231   27550]\n",
      " [      0       0       0       0       0       0   23827 1593690]] [[  47640    1330     216     373       0       0       0       0]\n",
      " [    537  565817    6826    4002       0       0       0       0]\n",
      " [    134    9280  231990     537       0       0       0       0]\n",
      " [    102    3228     484 1046186       0       0       0       0]\n",
      " [      0       0       0       0  389604    8384       0       0]\n",
      " [      0       0       0       0    8777 1022861       0       0]\n",
      " [      0       0       0       0       0       0  290043    6267]\n",
      " [      0       0       0       0       0       0    5329  341282]] [[  47556    1427     223     353       0       0       0       0]\n",
      " [    511  565760    6939    3972       0       0       0       0]\n",
      " [    132    9117  232183     510       0       0       0       0]\n",
      " [    104    3112     535 1046249       0       0       0       0]\n",
      " [      0       0       0       0  389559    8429       0       0]\n",
      " [      0       0       0       0    9046 1022591       0       0]\n",
      " [      0       0       0       0       0       0  290181    6130]\n",
      " [      0       0       0       0       0       0    5494  341116]]\n"
     ]
    }
   ],
   "source": [
    "model = lgb.Booster(model_file='models/lightgbm_sso_1.model')\n",
    "pred_train = model.predict(Xtrain)\n",
    "pred_dev = model.predict(Xdev)\n",
    "pred_test = model.predict(Xtest)\n",
    "\n",
    "preds_train = pred_train\n",
    "preds_dev = pred_dev\n",
    "preds_test = pred_test\n",
    "\n",
    "pred_train, pred_dev, pred_test = [], [], []\n",
    "\n",
    "preds1 = [preds_train, preds_dev, preds_test]\n",
    "preds2 = [pred_train, pred_dev, pred_test]\n",
    "\n",
    "for pred, predictions in zip(preds1, preds2):\n",
    "    for x in pred:\n",
    "        predictions.append(np.argmax(x))\n",
    "\n",
    "\n",
    "print(\n",
    "    metrics.accuracy_score(ytrain, pred_train),\n",
    "    metrics.accuracy_score(ydev, pred_dev),\n",
    "    metrics.accuracy_score(ytest, pred_test)\n",
    ")\n",
    "\n",
    "print(\n",
    "    metrics.classification_report(ytrain, pred_train),\n",
    "    metrics.classification_report(ydev, pred_dev),\n",
    "    metrics.classification_report(ytest, pred_test)\n",
    ")\n",
    "\n",
    "print(\n",
    "    metrics.confusion_matrix(ytrain, pred_train),\n",
    "    metrics.confusion_matrix(ydev, pred_dev),\n",
    "    metrics.confusion_matrix(ytest, pred_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 222905    5856     808    1705       0       0       0       0]\n",
      " [   2296 2641452   31868   17898       0       0       0       0]\n",
      " [    572   41627 1084434    2428       0       0       0       0]\n",
      " [    469   14768    2243 4882520       0       0       0       0]\n",
      " [      0       0       0       0 1819398   37879       0       0]\n",
      " [      0       0       0       0   40116 4774193       0       0]\n",
      " [      0       0       0       0       0       0 1355231   27550]\n",
      " [      0       0       0       0       0       0   23827 1593690]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(le.inverse_transform(ytrain), le.inverse_transform(pred_train), le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>آ</th>\n",
       "      <th>أ</th>\n",
       "      <th>إ</th>\n",
       "      <th>ا</th>\n",
       "      <th>ة</th>\n",
       "      <th>ه</th>\n",
       "      <th>ى</th>\n",
       "      <th>ي</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>آ</th>\n",
       "      <td>47556</td>\n",
       "      <td>1427</td>\n",
       "      <td>223</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>أ</th>\n",
       "      <td>511</td>\n",
       "      <td>565760</td>\n",
       "      <td>6939</td>\n",
       "      <td>3972</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>إ</th>\n",
       "      <td>132</td>\n",
       "      <td>9117</td>\n",
       "      <td>232183</td>\n",
       "      <td>510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ا</th>\n",
       "      <td>104</td>\n",
       "      <td>3112</td>\n",
       "      <td>535</td>\n",
       "      <td>1046249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ة</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>389559</td>\n",
       "      <td>8429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ه</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9046</td>\n",
       "      <td>1022591</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ى</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>290181</td>\n",
       "      <td>6130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ي</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5494</td>\n",
       "      <td>341116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       آ       أ       إ        ا       ة        ه       ى       ي\n",
       "آ  47556    1427     223      353       0        0       0       0\n",
       "أ    511  565760    6939     3972       0        0       0       0\n",
       "إ    132    9117  232183      510       0        0       0       0\n",
       "ا    104    3112     535  1046249       0        0       0       0\n",
       "ة      0       0       0        0  389559     8429       0       0\n",
       "ه      0       0       0        0    9046  1022591       0       0\n",
       "ى      0       0       0        0       0        0  290181    6130\n",
       "ي      0       0       0        0       0        0    5494  341116"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metrics.confusion_matrix(ytest, pred_test), columns=le.classes_, index=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The followng code deals with standardizing non-standard segmenter data.\n",
    "\n",
    "- `data/raw/test1.tsv` -> Standard Segmenter data. Al Mannar Corpus\n",
    "- `data/raw/test1_sso.tsv` -> The above data which has been artificially non-standardized\n",
    "\n",
    "Let us now standardize the artificially non-standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This converts the non-standardized segmentation test file into fc1 features\n",
    "so that it can be artificially standardized again.\n",
    "\"\"\"\n",
    "filename = \"data/raw/test1_sso.tsv\"\n",
    "fc = 'fc1'\n",
    "outfilename = \"data/sso/processed/segtest1_fc1.tsv\"\n",
    "\n",
    "symbols = set(('ا', 'ه'))\n",
    "sso = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "problem_lines = []\n",
    "\n",
    "with open(outfilename, 'w') as outfile:\n",
    "    fwriter = csv.writer(outfile, delimiter='\\t')\n",
    "    fwriter.writerow(['sentence_no', 'word_no', 'char_no'] + feature_codes[fc])\n",
    "    for i, sent in enumerate(sso['raw']):\n",
    "        try:\n",
    "            sent = sent.split()\n",
    "            for word_pos, word in enumerate(sent):\n",
    "                for char_pos, char in enumerate(word):\n",
    "                    if char in symbols or (char_pos == len(word)-1 and char == 'ى'):\n",
    "                        features = char_to_features(char_pos, word_pos, sent, feature_codes[fc])\n",
    "                        #target = [sso['standard'][i].split()[word_pos][char_pos]]\n",
    "                        fwriter.writerow([i, word_pos, char_pos] + features)\n",
    "        except AttributeError:\n",
    "            problem_lines.append(i)\n",
    "\n",
    "print(problem_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are three files namely:\n",
    "1. `data/raw/test1.tsv` -> Standard Segmenter data. All Mannar Corpus\n",
    "2. `data/raw/test1_sso.tsv` -> The above data which has been artificially non-standardized (We also call it **sso** in the code)\n",
    "3. `data/sso/processed/segtest1_fc1.tsv` -> SSO features generated from lgb model from (2) [We also call it **segtest** in the code]\n",
    "4. In the following code, our goal is create a new `overall_test.tsv` frame which we will call overall_test in the code. This frame will have the following columns:\n",
    "    - `original_raw` (The original sentence from the Al-Mannar Corpus)\n",
    "    - `original_seg` (Original gold annotations from Al-Mannar Corpus)\n",
    "    - `sso_raw` (Artificially SSO'd sentences)\n",
    "    - `sso_seg` (Artificially SSO'd annotations)\n",
    "    - `sso_raw_standardized` (sso_raw sentences which have been standardized by the lgbm standardizer)\n",
    "    - `sso_raw_standardized_segmented` (Output produced when Catboost segmenter is run on *sso_raw_standardized*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsayyed/Software/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Load the label encoder and the lgbm model for sso\n",
    "file = 'data/combined/processed/test1_sso_fc1.tsv'\n",
    "segtest = pd.read_csv(file, sep='\\t')\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.load('models/sso_encoder_classes.npy')\n",
    "model = lgb.Booster(model_file='data/standardizer/models/lgbm_set1_1500.model')\n",
    "\n",
    "# Make predictions using segtest. Add a new column to segtest called target \n",
    "segtestframe = segtest[feature_codes['fc1']]\n",
    "for col in segtestframe.columns[1:]:\n",
    "    segtestframe[col] = segtestframe[col].astype('category')\n",
    "predictions = model.predict(segtestframe)\n",
    "target = []\n",
    "for pred in predictions:\n",
    "    target.append(np.argmax(pred))\n",
    "target = le.inverse_transform(target)\n",
    "segtest['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsayyed/Software/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/zsayyed/Software/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# The following block of code will standardize substandard sentences.\n",
    "# Our main targets are test1_sso.tsv and test2_sso.tsv. They are both \n",
    "# present in data/combined/raw. Their corresponding feature files are \n",
    "# have been created for the segmented and are present in data/combined/processed \n",
    "# Since, the segmenter and the standardizer both use the same feauture sets, \n",
    "# we can use it for out purposes.\n",
    "def standardize(inframe, model, label_encoder, feature_code='fc1'):\n",
    "    \"\"\"\n",
    "    inframe: The input feature frame\n",
    "    Returns: A column consisting of standardized sentences.\n",
    "    \"\"\"\n",
    "    segtestframe = inframe[feature_codes[feature_code]]\n",
    "    for col in segtestframe.columns[1:]:\n",
    "        segtestframe[col] = segtestframe[col].astype('category')\n",
    "    predictions = model.predict(segtestframe)\n",
    "    target = []\n",
    "    for pred in predictions:\n",
    "        target.append(np.argmax(pred))\n",
    "    target = label_encoder.inverse_transform(target)\n",
    "    inframe['predictions'] = target\n",
    "\n",
    "    inframe['next_char_word_no'] = inframe['word_no'].shift(-1)\n",
    "    inframe['replace_alif_ta'] = (inframe['focus'] == 'ه') | (inframe['focus'] == 'ا')\n",
    "    inframe['last_char'] = inframe['word_no'] != inframe['next_char_word_no']\n",
    "    inframe['replace_ya'] = inframe['last_char'] & (inframe['focus'] == 'ى')\n",
    "    inframe['really_replace'] = inframe['replace_alif_ta'] | inframe['replace_ya']\n",
    "    inframe['standardized_char'] = inframe['focus']\n",
    "    inframe['standardized_char'][inframe['really_replace'] == True] = inframe['predictions'][inframe['really_replace'] == True]\n",
    "    words = inframe.groupby(['file', 'sentence_no', 'word_no'])['standardized_char'].apply(lambda x: ''.join(list(x)))\n",
    "    sentences = words.groupby(['file', 'sentence_no']).apply(lambda x: ' '.join(list(x)).strip())\n",
    "    return sentences\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.load('models/sso_encoder_classes.npy')\n",
    "model = lgb.Booster(model_file='data/standardizer/models/lgbm_set1_1500.model')\n",
    "inframe = pd.read_csv('data/combined/processed/test1_sso_fc1.tsv', sep='\\t')\n",
    "test1std = standardize(inframe, model, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Software/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/util.pxd\u001b[0m in \u001b[0;36mutil.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-cba14085ba2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest1std\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'P607'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Software/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m    980\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.BaseMultiIndexCodesEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file'"
     ]
    }
   ],
   "source": [
    "test1std['file'] == 'P607'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1base = pd.read_csv('data/combined/raw/test1_sso.tsv', sep='\\t')\n",
    "test1base['sso_raw_standardized'] = test1std.values\n",
    "test1base.to_csv('data/combined/results/test1result.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2base = pd.read_csv('data/combined/raw/test2_sso.tsv', sep='\\t')\n",
    "test2base['sso_raw_standardized'] = test2std.values\n",
    "test2base.to_csv('data/combined/results/test2result.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1844, 7)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_orig = pd.read_csv('data/raw/test1.tsv', sep='\\t')\n",
    "overall_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_test['file_name'] = sso['file']\n",
    "overall_test['sent_no'] = sso['sentence']\n",
    "overall_test['original_raw'] = test1_orig['raw']\n",
    "overall_test['original_seg'] = test1_orig['seg']\n",
    "overall_test['sso_raw'] = sso['raw']\n",
    "overall_test['sso_seg'] = sso['seg']\n",
    "overall_test['standardized_sso_raw'] = sso['raw_standardized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_test.to_csv(\"data/overall_test.tsv\", '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(sent, le, model, fc):    \n",
    "    symbols = set(('ا', 'ه'))\n",
    "    sent = sent.split()\n",
    "    standard_sent = []\n",
    "    for word_pos, word in enumerate(sent):\n",
    "        standard_word = \"\"\n",
    "        for char_pos, char in enumerate(word):\n",
    "            if char in symbols or (char_pos == len(word)-1 and char=='ى'):\n",
    "                test_frame = pd.DataFrame(columns=feature_codes[fc])\n",
    "                #test_frame['chr_position'] = test_frame['chr_position'].astype('int64')\n",
    "                for col in test_frame.columns:\n",
    "                    test_frame[col] = test_frame[col].astype(Xtrain[col].dtype)\n",
    "                print(test_frame.dtypes)\n",
    "                features = char_to_features(char_pos, word_pos, sent, feature_codes[fc])\n",
    "                test_frame.loc[len(test_frame)] = features\n",
    "                print(test_frame)\n",
    "                # print(np.array(features).reshape(1, len(features)))\n",
    "                target = np.argmax(model.predict(test_frame))\n",
    "                target = le.inverse_transform(target)\n",
    "            else:\n",
    "                target = char\n",
    "            standard_word += target\n",
    "        standard_sent.append(word)\n",
    "    return \" \".join(standard_sent)\n",
    "    \n",
    "    \n",
    "# The following block of code will standardize substandard sentences.\n",
    "# Our main targets are test1_sso.tsv and test2_sso.tsv. They are both \n",
    "# present in data/combined/raw. Their corresponding feature files are \n",
    "# have been created for the segmented and are present in data/combined/processed \n",
    "# Since, the segmenter and the standardizer both use the same feauture sets, \n",
    "# we can use it for out purposes.\n",
    "def standardize(inframe, model, label_encoder, feature_code='fc1'):\n",
    "    \"\"\"\n",
    "    inframe: The input feature frame\n",
    "    Returns: A column consisting of standardized sentences.\n",
    "    \"\"\"\n",
    "    segtestframe = inframe[feature_codes[feature_code]]\n",
    "    for col in segtestframe.columns[1:]:\n",
    "        segtestframe[col] = segtestframe[col].astype('category')\n",
    "    predictions = model.predict(segtestframe)\n",
    "    target = []\n",
    "    for pred in predictions:\n",
    "        target.append(np.argmax(pred))\n",
    "    target = label_encoder.inverse_transform(target)\n",
    "    inframe['predictions'] = target\n",
    "\n",
    "    inframe['sso_char'] = inframe['focus']\n",
    "    print(inframe.shape)\n",
    "    for i in tqdm(range(len(inframe) - 1)):\n",
    "        if ((inframe['focus'][i] == 'ى' and inframe['word_no'][i] != inframe['word_no'][i+1])\n",
    "            or inframe['focus'][i] == 'ه'\n",
    "            or inframe['focus'][i] == 'ا'):\n",
    "            inframe['sso_char'][i] = inframe['predictions'][i]\n",
    "    words = inframe.groupby(['file', 'sentence_no', 'word_no'])['sso_char'].apply(lambda x: ''.join(list(x)))\n",
    "    return words.groupby(['file', 'sentence_no']).apply(lambda x: ' '.join(list(x)).strip())\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.load('models/sso_encoder_classes.npy')\n",
    "model = lgb.Booster(model_file='data/standardizer/models/lgbm_set1_1500.model')\n",
    "inframe = pd.read_csv('data/combined/processed/test2_sso_fc1.tsv', sep='\\t')\n",
    "test2std = standardize(inframe, model, le)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sso = pd.read_csv('data/combined/raw/test1_sso.tsv', sep='\\t')\n",
    "sso['raw_standardized'] = sso['sso_raw']\n",
    "\n",
    "for i in range(len(segtest)):\n",
    "    sentence_no = segtest['sentence_no'][i]\n",
    "    word_no = segtest['word_no'][i]\n",
    "    char_no = segtest['chr_position'][i]\n",
    "    target = segtest['target'][i]\n",
    "    if segtest['focus'][i] != segtest['target'][i]:\n",
    "        rs_sent = sso['raw_standardized'][sentence_no].split()\n",
    "        rs_sent[word_no] = rs_sent[word_no][:char_no] + target + rs_sent[word_no][char_no+1:]\n",
    "        sso['raw_standardized'][sentence_no] = \" \".join(rs_sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
